\include{Sweave}
\documentclass [a4paper]{article}
%\VignetteIndexEntry{qfa}
\usepackage{url}
\usepackage{hyperref}            
\title{qfaBayes - An R package for Bayesian Quantitative Fitness Analysis}
\author{Jonathan Heydari}
\begin{document}
\setlength\parindent{0pt}
\maketitle

\section{Introduction}

Quantitative Fitness Analysis (QFA) is an experimental and computational workflow for comparing fitnesses of microbial cultures grown in parallel.  QFA can be applied to focused observations of single cultures but is most useful for genome-wide genetic interaction or drug screens investigating up to thousands of independent cultures.  The central experimental method is the inoculation of independent, dilute liquid microbial cultures onto solid agar plates which are incubated and regularly photographed.  Photographs from each time-point are analyzed, producing quantitative cell density estimates, which are used to construct growth curves, allowing quantitative fitness measures to be derived.  Culture fitnesses can be compared to quantify and rank genetic interaction strengths or drug sensitivities. The effect on culture fitness of any treatments added into substrate agar (e.g. small molecules, antibiotics or nutrients) or applied to plates externally (e.g. UV irradiation, temperature) can be quantified by QFA.

Detailed descriptions of how to carry out QFA experiments are available in open access articles, particularly in \href{http://dx.doi.org/10.3791/4018}{Banks et al. (2012)} and  \href{http://dx.doi.org/10.1371/journal.pgen.1001362}{Addinall et al. (2011)}.  The purpose of this document is to describe, in detail, some of the computational methods available in the qfaBayes R package for summarising experimentally observed growth curves during QFA, and to demonstrate the computational component of QFA using some small, example datasets.

\section{QFA data}

The raw experimental data generated by QFA consists of timeseries photographs of cultures growing on agar plates.  The first step in the computational component of the QFA workflow is to convert these photographic observations into cell density estimates for cultures in each position on each plate analysed.  The Colonyzer image analysis tool (\href{http://dx.doi.org/10.1186/1471-2105-11-287}{Lawless et al. (2010)}) is designed for this task and can be downloaded from its \href{http://research.ncl.ac.uk/colonyzer/}{website}.  Once all the images have been successfully analysed, the next step is to use the qfaBayes R package to associate culture locations with genotypes and to construct growth curves (cell density estimates over time) for each culture.

\section{Installing the qfaBayes package}

The qfaBayes package source code is available for download from \href{http://r-forge.r-project.org/projects/qfa}{R-Forge}, and so it should be possible to install the latest version using the R package management system on a wide range of operating systems by executing the following command within an R environment: 
\begin{verbatim}
install.packages("qfaBayes",repos="http://r-forge.r-project.org")
\end{verbatim}

Once installed, the package can be loaded ready for use with
\begin{verbatim}
library(qfaBayes)
\end{verbatim}

Please note that this installation method will typically only work using the latest version of R (which can be freely downloaded from the R \href{http://www.r-project.org/}{website}).  Alternatively, instructions for accessing the source code for the package from are available \href{http://r-forge.r-project.org/scm/?group_id=880}{here}.

\section{Function documentation}

The following command will provide an overview of functions available within the qfaBayes package together with brief descriptions of what they do and links to detailed descriptions indicating input arguments and output:
\begin{verbatim}
help(package="qfaBayes")
\end{verbatim}
This document can be accessed at any time with:
\begin{verbatim}
vignette("qfaBayes")
\end{verbatim}

\section{General overview}

This R package is intended to fit the three Bayesian Models described in J Heydari, C Lawless, D Lydall and D J Wilkinson. \emph{Bayesian hierarchical modelling for inferring genetic interactions in yeast. Journal of the Royal Statistical Society: Series C (Applied Statistics)}, in submission.
The three Bayesian models are the SHM (Single hierarchical model), IHM (Interacation hierarchical model), and JHM (Joint hierarchical model).
There are six demos included in this packge, three of which (\verb$SHM_simple_C$, \verb$IHM_simple_C$ and \verb$JHM_simple_c$)have been made for ease of use, hiding all post-processing. 
The other three (\verb$SHM_C$, \verb$IHM_C$ and \verb$JHM_C$) consist of many lines of code that can be more easily edited to create a much more tailored post-processing.
Variables \verb$burn$, \verb$iter$ and \verb$thin$ control the burn-in period, output sample size and thinning.
These are all set to 1 in all the six of the demo's so that a user can get familiar with the workflow first, it is essential that you change them to much larger numbers for sufficient convergence to happen.
Typically the models need a computational time that ranges from range from days to weeks.

\subsection{SHM}
\verb$SHM_simple_C$
This demo runs the SHM for the $\emph{ura3}\Delta$ data set at $27\,^{\circ}\mathrm{C}$ trimmed to only include plate 15, which accounts for only 50 of the 4294 $\emph{orf}\Delta$s avalible.
It is expected that you will wish to create ouput with larger sample size, burn in period and thinning, to do this simply copy all the code from the demo and change the variables \verb$burn$, \verb$iter$ and \verb$thin$.

The typical model computation time of the SHM alone is one week with burn-in of 800000, sample size of 10000 and thinning of 10.

A more detailed workflow which is compatible with the standalone C code is given in demo \verb$SHM_C$.

\subsection{IHM}
\verb$IHM_simple_C$
This demo runs the SHM for the $\emph{ura3}\Delta$ data set at $27\,^{\circ}\mathrm{C}$, trimmed to only include plate 15, which accounts for only 50 of the 4294 $\emph{orf}\Delta$s available.
Next, the SHM is then run for the $\emph{cdc13-1}\Delta$ data set at $27\,^{\circ}\mathrm{C}$, trimmed to only include plate 15, which accounts for only 50 of the 4294 $\emph{orf}\Delta$s available.
The IHM is then run using the output from the SHM output for the two data sets.
Finally you will be asked if you wish to create a fitness plot with your results.

It is expected that you will wish to create ouput with larger sample size, burn in period and thinning, to do this simply copy all the code from the demo and change the variables \verb$burn$, \verb$iter$ and \verb$thin$ where they appear.
The typical model computation time of the IHM alone is one day with burn-in of 800000, sample size of 10000 and thinning of 10.

A more detailed workflow which is compatible with the standalone C code is given in demo \verb$IHM_C$.

\subsection{JHM}
\verb$JHM_simple_C$
This demo runs the JHM for the $\emph{ura3}\Delta$ and $\emph{cdc13-1}\Delta$ data sets, both at $27\,^{\circ}\mathrm{C}$, trimmed to only include plate 15, which accounts for only 50 of the 4294 $\emph{orf}\Delta$s available.
At the end of the demo you will be asked if you wish to create a fitness plot with your results.

It is expected that you will wish to create ouput with larger sample size, burn in period and thinning, to do this simply copy all the code from the demo and change the variables \verb$burn$, \verb$iter$ and \verb$thin$ where they appear.
The typical model computation time of the JHM alone is two weeks with burn-in of 800000, sample size of 5000 and thinning of 10.

A more detailed workflow which is compatible with the standalone C code is given in demo \verb$JHM_C$.

\section{SHM$\_$simple$\_$C Demo}
To see how the \verb$SHM_simple_C$ demo works the following detailed explaination is provided and is to be used in conjunction with the R package manual.
The \verb$SHM_simple_C$ demo runs the following commands: \\
\\
\verb$> data("URA3_Raw_extratrim_15")$\\
\verb$> a<-a_15$\\
\\
The above commands load a trimmed ura3Del data set consisting of only of master plate 15 to the variable \verb$a_15$ and copys this to a variable "a".
\\
\\
\verb$> data("priors_SHM")$\\
\verb$> PRIORS=as.double((priors_SHM)[[1]])[1:18]$\\
\\
The above commands loads a broad set of prior values for the SHM to the variable \verb$priors_SHM$ and then copys this to variable PRIORS.
\\
\\
\verb$> qfa.variables(a)$\\
\\
The above command lists the avalible options for the treatment, screen number and master plate.
\\
\\
\verb@> Screen<-unique(a$Screen.Name)@\\
\\
The above command selects all screens for the following SHM analysis.
\\
\\
\verb$> SHM<-SHM_postpro(a=a,Treat=27,Screen=Screen,MPlate=15)$\\
\\
The above command runs the post processing for the SHM, organising the Colonyser ouput in variable \verb$a$ for use with the SHM.
The dataset will be trimmed to only include the data corresponding to the specified \verb$Treat$, \verb$Screen$ and \verb$MPlate$.
\\
\\
\verb@> SHM_output<-SHM_main(burn=1,iters=1,thin=1,CAPL=50,@\\
\verb@QFA.I=SHM$QFA.I,QFA.y=SHM$QFA.y,QFA.x=SHM$QFA.x,@\\
\verb@QFA.NoORF=SHM$QFA.NoORF,QFA.NoTIME=SHM$QFA.NoTIME,PRIORS=PRIORS)@\\
\\
The above command runs the C code for the SHM in R. \verb$burn$, \verb$iters$ and \verb$thin$ which have all been set to 1 for the purpose of the demo.
\verb$CAPL$ is the number of orfs for the SHM to proform the analysis on.
The output \verb$SHM_output$ will be a table of posterior samples, where each coloumn corresponds to a different model paramater; a table header is included to idendify each coloumn.
\\
\verb@> ask_plot_simple()@\\
\\
The above command will ask the user if they would like an example of some plots created form the posterior sample.
\\
\verb@> filename="DEMO"@\\
\verb@> pdf(paste("SHM_plot_",filename,".pdf",sep=""),useDingbats=F)@\\
\verb@> plot_SHM_simple(SHM_output,SHM)@\\
\verb@> dev.off()@\\
\\
The above commands save fitness plots for each ORF to a pdf file, where black is for the repeat fitted curves and red for orf level fitted curves.
\end{document}
% eof
