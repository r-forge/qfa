\include{Sweave}
\documentclass [a4paper]{article}
%\VignetteIndexEntry{qfa}
\usepackage{url}
\usepackage{hyperref}            
\title{qfaBayes - An R package for Bayesian Quantitative Fitness Analysis}
\author{Jonathan Heydari}
\begin{document}
\setlength\parindent{0pt}
\maketitle

\section{Introduction}

Quantitative Fitness Analysis (QFA) is an experimental and computational workflow for comparing fitnesses of microbial cultures grown in parallel.  QFA can be applied to focused observations of single cultures but is most useful for genome-wide genetic interaction or drug screens investigating up to thousands of independent cultures.  The central experimental method is the inoculation of independent, dilute liquid microbial cultures onto solid agar plates which are incubated and regularly photographed.  Photographs from each time-point are analyzed, producing quantitative cell density estimates, which are used to construct growth curves, allowing quantitative fitness measures to be derived.  Culture fitnesses can be compared to quantify and rank genetic interaction strengths or drug sensitivities. The effect on culture fitness of any treatments added into substrate agar (e.g. small molecules, antibiotics or nutrients) or applied to plates externally (e.g. UV irradiation, temperature) can be quantified by QFA.

Detailed descriptions of how to carry out QFA experiments are available in open access articles, particularly in \href{http://dx.doi.org/10.3791/4018}{Banks et al. (2012)} and  \href{http://dx.doi.org/10.1371/journal.pgen.1001362}{Addinall et al. (2011)}.  The purpose of this document is to describe, in detail, some of the computational methods available in the qfaBayes R package for summarising experimentally observed growth curves during QFA, and to demonstrate the computational component of QFA using some small, example datasets.

\section{QFA data}

The raw experimental data generated by QFA consists of timeseries photographs of cultures growing on agar plates.  The first step in the computational component of the QFA workflow is to convert these photographic observations into cell density estimates for cultures in each position on each plate analysed.  The Colonyzer image analysis tool (\href{http://dx.doi.org/10.1186/1471-2105-11-287}{Lawless et al. (2010)}) is designed for this task and can be downloaded from its \href{http://research.ncl.ac.uk/colonyzer/}{website}.  Once all the images have been successfully analysed, the next step is to use the qfaBayes R package to associate culture locations with genotypes and to construct growth curves (cell density estimates over time) for each culture.

\section{Installing the qfaBayes package}

The qfaBayes package source code is available for download from \href{http://r-forge.r-project.org/projects/qfa}{R-Forge}, and so it should be possible to install the latest version using the R package management system on a wide range of operating systems by executing the following command within an R environment: 
\begin{verbatim}
install.packages("qfaBayes",repos="http://r-forge.r-project.org")
\end{verbatim}

Once installed, the package can be loaded ready for use with
\begin{verbatim}
library(qfaBayes)
\end{verbatim}

Please note that this installation method will typically only work using the latest version of R (which can be freely downloaded from the R \href{http://www.r-project.org/}{website}).  Alternatively, instructions for accessing the source code for the package from are available \href{http://r-forge.r-project.org/scm/?group_id=880}{here}.

\section{Function documentation}

The following command will provide an overview of functions available within the qfaBayes package together with brief descriptions of what they do and links to detailed descriptions indicating input arguments and output:
\begin{verbatim}
help(package="qfaBayes")
\end{verbatim}
This document can be accessed at any time with:
\begin{verbatim}
vignette("qfaBayes")
\end{verbatim}
Documentation for specific functions can be obtained using the usual R mechanisms. For example, help on the function \verb$colonyzer.read$ can be obtained with:
\begin{verbatim}
?colonyzer.read
\end{verbatim}
There is a short demo script comparing QFA of \textit{cdc13-1} and \textit{ura3$\Delta$} at $27\,^{\circ}\mathrm{C}$ to infer genes interacting with the telomere cap.  Note that the curve-fitting functions can throw up many error messages which can be ignored.  These occur when attempting to fit the model to missing cultures.  This demo only contains data from the yeast deletion collection describing one plate out of a possible 15.  This demo also only uses one replicate plate out of a possible eight for each screen in order to save time.  To repeat the analysis with all available replicates, uncomment the appropriate lines in the demo script.  Note that the full analysis takes approximately 30 mins.  The demo can be loaded with:
\begin{verbatim}
demo("telomereCap")
\end{verbatim}
\end{document}

\section{General overview}

This R package is inteneded to fit the three Bayesian Models described in J Heydari, C Lawless, D Lydall and D J Wilkinson. \emph{Bayesian hierarchical modelling for inferring genetic interactions in yeast. Journal of the Royal Statistical Society: Series C (Applied Statistics)}, in submission.
The three bayesian models are the SHM (Single hierarchical model), IHM (Interacicon hierarchical model), and JHM (Joint hierarchical model).
There are six demos included in this pacakge, three of which (\texttt{SHM_simple_C}, \texttt{IHM_simple_C} and \texttt{JHM_simple_c})have been made for ease of use, hiding all postprocessing. 
The other three (\texttt{SHM_C}, \texttt{IHM_C} and \texttt{JHM_C}) consist of many lines of code that  can be more easily edited to create a much more tailiered post processing.
Variables \verb$burn$,\verb$iter$ and \verb$thin$ control the burn-in period, output sample size and thinning.
These are all set to 1 in all the six of the demo's so that a user can get familier with the workflow first, it is essential that you change them to much larger numbers for sufficent converagance to happen.
Typically the models need a computational time that ranges from range from days to weeks.

\subsection{SHM}
\verb$SHM_simple_C$
This demo runs the SHM for the \emph{ura3}$Delta$ data set trimmed to only include plate 15, which accounts for only 50 of the 4294 \emph{orf}$Delta$s avalible.
It is expected that you will wish to create ouput with larger sample size, burn in period and thinning, to do this simply copy all the code from the demo and change the variables \verb$burn$,\verb$iter$ and \verb$thin$.

The typical model computation time of the SHM alone is one week with burn-in of 800000, sample size of 10000 and thinning of 10.

A more detailed workflow which is compatable with the standalone C code is given in demo \verb$SHM_C$.

\subsection{IHM}
\verb$IHM_simple_C$
This demo runs the SHM for the \emph{ura3}$Delta$ data set, trimmed to only include plate 15, which accounts for only 50 of the 4294 \emph{orf}$Delta$s avalible.
Next, the SHM is then run for the \emph{cdc13-1}$Delta$ data set, trimmed to only include plate 15, which accounts for only 50 of the 4294 \emph{orf}$Delta$s avalible.
The IHM is then run using the output from the SHM output for the two data sets.
Finally you will be asked if you wish to create a fitness plot with your results.

It is expected that you will wish to create ouput with larger sample size, burn in period and thinning, to do this simply copy all the code from the demo and change the variables \verb$burn$,\verb$iter$ and \verb$thin$ where they apear.
The typical model computation time of the IHM alone is one day with burn-in of 800000, sample size of 10000 and thinning of 10.

A more detailed workflow which is compatable with the standalone C code is given in demo \verb$IHM_C$.

\subsection{JHM}
\verb$JHM_simple_C$
This demo runs the JHM for the \emph{ura3}$Delta$ and \emph{cdc13-1}$Delta$ data set, trimmed to only include plate 15, which accounts for only 50 of the 4294 \emph{orf}$Delta$s avalible.
At the end of the demo you will be asked if you wish to create a fitness plot with your results.

It is expected that you will wish to create ouput with larger sample size, burn in period and thinning, to do this simply copy all the code from the demo and change the variables \verb$burn$,\verb$iter$ and \verb$thin$ where they apear.
The typical model computation time of the JHM alone is two weeks with burn-in of 800000, sample size of 5000 and thinning of 10.

A more detailed workflow which is compatable with the standalone C code is given in demo \verb$JHM_C$.

% eof
